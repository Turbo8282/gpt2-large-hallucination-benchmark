{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "297d5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f591e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee8e0cdd46e48d4b24564185c9e7af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90c905405f440d289b58f40cac2edef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3349fac8c6a45388da3183fcb424b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4d2b8fb0d14b67b445e8bfa0b29875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f91472a6254c4798540a5a8f68d86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9487d3330e4572bb53ce0ab257e58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fa129de8944a50a5022352ffd72720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11694de",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df591ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, but what does that really mean?\\n\\nI'll give you an example. Suppose you have a language model, and you want to model a language. You might want to model a language by analyzing a collection of words, or by analyzing an ordered list of words, or by analyzing a sentence.\\n\\nYou might also want to model a language by analyzing a sentence with a set of input words. These are called input models.\\n\\nYou might also want to model a language by analyzing a sentence with a set of input words. These are called output models.\\n\\nThere are many different ways to model a language. You might want to model the language by analyzing a sentence. Or you might want to model the language by analyzing a sentence and an ordered list of input words. Or you might want to model the language by analyzing a sentence and an ordered list of output words.\\n\\nLet's look at some examples.\\n\\nInput models\\n\\nLet's take a look at an input model. Suppose we have a sentence, a sentence-list, and a sentence-output. We can analyze this sentence.\\n\\nLet's say we want to model this sentence by analyzing a sentence-output and a sentence-input.\\n\\nLet's say we want\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a language designer. How can I help you?\\n\\nWhat is a language model?\\n\\nA language model is a language that you can use to model the design of your code. For example, you can use a language model to design a language (the language you use to describe the design of your application).\\n\\nWhat is a language model?\\n\\nA language model is a language that you can use to model the design of your code.\\n\\nWhy should I use a language model?\\n\\nBecause a language model lets you model the design of your code. This lets you:\\n\\nWrite code that is easy to reason about.\\n\\nWrite code that is easier to test.\\n\\nWrite code that is easier to refactor.\\n\\nWrite code that is easier to understand.\\n\\nWrite code that is easier to run.\\n\\nWhy should I don't use a language model?\\n\\nA language model is not an important part of a programming language. It's usually not even required. This is because a language model can be used to describe the design of your application, not the implementation.\\n\\nWhat is a language model not?\\n\\nA language model is not a language used to describe how you design your code.\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'll talk about how I've built a simple language model in Haskell.\\n\\nThis post has been translated from a webinar I did in November 2014.\\n\\nI hope you find it interesting. If you have any questions, then please ask!\\n\\nAdvertisements\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a language designer, so you're not going to find me teaching you how to write a language, but I'm going to show you how to write a language.\\n\\nI will give you a language in which you can write a program that does the following:\\n\\nWrite a language (more on that in a moment)\\n\\nWrite a program that translates a language into English\\n\\nWrite a program that translates a program to English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\nWrite a program that translates a program into English\\n\\n\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, I\\'m not a language designer. I\\'m just an idiot.\"\\n\\n\"I don\\'t see how it\\'s possible to develop a good language model,\" I replied.\\n\\n\"You\\'re not sure? That\\'s how I have to explain it to you.\"\\n\\n\"But I can\\'t be a language designer,\" I said. \"How am I going to say this in English?\"\\n\\n\"You can,\" he said. \"It\\'s called a language model.\"\\n\\nThat was a shock. I had been thinking that the only way to learn a language is to be taught it, and to learn it in a way that makes it easy for me to understand. But then I realized that my language model would have to be a language model that I could understand. I had forgotten how hard that was.\\n\\nIn contrast to my own experience, I had never given up on learning languages. For most of my life, I had been working on a language model, but it had never been able to produce anything meaningful. The only thing that had come out of it was a set of rules that I could use to determine whether a statement was true or false, and a set of rules that I could use to determine whether a sentence should be translated to another'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9103a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"how many episodes in season 2 breaking bad?\\n\\nNo, the actual number is two. One was the pilot, which was the first episode of the show, and one was the last episode of the show, which was the finale. But you can't tell the number of episodes in season 2 because there's never a point in season 2 where these numbers are in any sort of chronological order.\\n\\nAdvertisement\\n\\nHow do you know the total number of episodes of Breaking Bad?\\n\\nI know at what point you'll be able to tell the total. It's like a magic number.\\n\\nWho is the most important person on this show?\\n\\nI'll tell you the most important person. The most important person on this show is Walter White.\\n\\nHow many episodes do you think he'll have by the end of this series?\\n\\nIt's a tricky question. I don't know. I'm not sure how many episodes he'll have. I don't know how many episodes he's going to have. I don't know how many episodes he's going to have. I don't know how many episodes he'll have. It's not even a number. It's a number of things that he's going to have. It's a number of different things that he's going\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"how many episodes in season 2 breaking bad?\", max_length=30, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "698b6144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"1+1 is what I call a 'naked' function because it does not need to return a value, it just works.\\n\\nBut the function you are trying to call is a nested function, so you must return a value.\\n\\n$this->nested($value);\\n\\nThe nested class is using the @_ constructor.\\n\\nSo just like you have a nested function in your function definition, you can use the @_ constructor in your class.\\n\\nfunction my_object_method( $name, $args = array() ) { // You must return a value }\\n\\nThe @_ constructor is a special constructor that is used to initialize a new instance of the class.\\n\\nSo now we have something that looks like a 'naked' function, but it uses the @_ constructor to initialize the class.\\n\\n$this->nested(@_);\\n\\nThe @_ constructor is very similar to the __construct() method, except that it is called at the point of creation of the class.\\n\\nSo now we have something that looks like a 'naked' function, but it uses the @_ constructor to initialize the class, just like we did in the 'naked' function.\\n\\n$this\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"1+1 is\", max_length=30, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9bd34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
